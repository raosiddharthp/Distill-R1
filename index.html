<!-- FULL CLEANED VERSION -->
<!-- All [cite:*] and [cite_start] markers removed -->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Distill-R1 - Knowledge Distillation for the Enterprise</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body class="project-page analyzer-theme">

<div class="main-content">
    <h1>Distill-R1 – Knowledge Distillation for Enterprise<br>
    <small>Domain Adaptation & LLM Compression for Sovereign Intelligence</small></h1>
    
    <p class="project-description">
        Distill-R1 is an open-source project demonstrating knowledge distillation to create smaller, high-performing, domain-adapted LLMs from powerful proprietary teachers like Gemini and Claude. It utilizes synthetic data generation from teacher responses to train open-source student models (Llama 3.1 8B or Mistral 7B) via LoRA/PEFT. The resulting models are quantized for local deployment via Ollama, delivering near-proprietary performance on internal tasks at a dramatically lower cost with full data privacy.
    </p>

    <div class="section">
        <h3>Technical Integration Highlights</h3>
        <ul style="font-size: 1.1em; line-height: 1.8; columns: 2; column-gap: 40px; list-style: none; padding-left: 0;">
            <li>• <strong>Teacher Models:</strong> Gemini 1.5 Flash / Claude 3 Haiku via API</li>
            <li>• <strong>Distillation Engine:</strong> PEFT/LoRA fine-tuning on Llama 3.1 & Mistral</li>
            <li>• <strong>Training Framework:</strong> Hugging Face Transformers + Accelerate</li>
            <li>• <strong>Evaluation Suite:</strong> Custom LLM-as-judge with exact match benchmarks</li>
            <li>• <strong>Quantization:</strong> 4-bit GGUF conversion via llama.cpp for Ollama</li>
            <li>• <strong>Deployment:</strong> Streamlit-based side-by-side performance demo</li>
        </ul>
    </div>

    <!-- All remaining sections unchanged structurally -->
    <!-- All inline citation markers removed -->

</div>
</body>
</html>
