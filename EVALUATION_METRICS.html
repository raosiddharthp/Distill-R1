<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Detailed Evaluation Metrics & Methodology – Distill-R1</title>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&display=swap');

    body {
      font-family: 'Space Mono', monospace;
      background: white;
      color: black;
      max-width: 900px;
      margin: 60px auto;
      padding: 0 20px;
      line-height: 1.7;
      font-size: 17px;
    }

    h1 {
      color: #0066FF;
      font-size: 2.4em;
      border-bottom: 4px solid #0066FF;
      padding-bottom: 15px;
      margin-bottom: 40px;
    }

    h2 {
      color: #0066FF;
      font-size: 1.8em;
      margin-top: 3em;
      border-bottom: 2px solid #0066FF;
      padding-bottom: 8px;
    }

    table {
      border-collapse: collapse;
      width: 100%;
      margin: 30px 0;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 12px;
      text-align: left;
    }

    th {
      background: #f0f8ff;
      color: #0066FF;
      font-weight: 700;
    }

    .back {
      margin-top: 80px;
      padding-top: 30px;
      border-top: 2px solid #0066FF;
    }
  </style>
</head>
<body>

<h1>Detailed Evaluation Metrics & Methodology</h1>

<p>Distill-R1 uses a rigorous, multi-dimensional evaluation framework to compare teacher and student models on enterprise-relevant tasks.</p>

<h2>Core Metrics</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Description</th>
      <th>Measurement Method</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Answer Accuracy</td>
      <td>Percentage of responses matching ground truth or judged correct</td>
      <td>LLM-as-judge (strong model) + exact match where possible</td>
      <td>> 85% of teacher</td>
    </tr>
    <tr>
      <td>Helpfulness Score</td>
      <td>Subjective quality rating</td>
      <td>LLM-as-judge on 1–10 scale</td>
      <td>> 8.5/10</td>
    </tr>
    <tr>
      <td>Hallucination Rate</td>
      <td>Percentage of responses with unsupported claims</td>
      <td>Self-checking + human spot-check</td>
      <td>< 10%</td>
    </tr>
    <tr>
      <td>Inference Latency</td>
      <td>Average response time</td>
      <td>Local measurement on reference hardware</td>
      <td>< 500ms</td>
    </tr>
    <tr>
      <td>Cost per 1k Tokens</td>
      <td>Inference cost</td>
      <td>API pricing vs local (free)</td>
      <td>$0.00 local</td>
    </tr>
  </tbody>
</table>

<h2>Evaluation Methodology</h2>

<ol>
  <li>Hold-out test set (10% of synthetic data) never seen during training</li>
  <li>Teacher model generates reference responses</li>
  <li>Student model generates responses on same queries</li>
  <li>Automated scoring with LLM-as-judge (strong model like Gemini Pro)</li>
  <li>Manual review of 10% sample for validation</li>
  <li>Performance comparison across accuracy, helpfulness, hallucination, latency</li>
</ol>

<h2>Benchmark Domains Tested</h2>

<ul>
  <li>IT Support Tickets</li>
  <li>Product Q&A (SaaS)</li>
  <li>Compliance & Policy Queries</li>
  <li>Financial Reporting Questions</li>
</ul>

<div class="back">
  <a href="index.html">← Back to Main Design</a>
</div>

</body>
</html>